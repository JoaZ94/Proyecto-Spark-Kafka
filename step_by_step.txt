# Ingresar a Kafka1p
docker exec -it kafka1p bash

# Crear t√≥pico en Kafka1p (una sola vez)
kafka-topics --bootstrap-server localhost:9092 --create --topic DataTopic --replication-factor 3 --partitions 3

# Eliminar topic Kafka1p
kafka-topics --bootstrap-server localhost:9092 --delete --topic DataTopic

#listar Topicos
kafka-topics --bootstrap-server localhost:9092 --list

Enviar datos desde Kafka1p como Producer
kafka-console-producer --bootstrap-server localhost:9092 --topic DataTopic

#Consumir mensajer en Kafka1p
kafka-console-consumer --bootstrap-server localhost:9092 --topic DataTopic --from-beginning

# Ingresar a Spark-Master
docker exec -it proyecto-spark-kafka-spark-1 bash

# Ejecutar Spark con las dependencias de Kafka (Spark-master)
spark-submit --packages org.apache.spark:spark-sql-kafka-0-10_2.12:3.1.2 /opt/spark/code/analysis.py



from pyspark.sql import SparkSession
from pyspark.sql.types import StructType, StructField, StringType, IntegerType, MapType
import pyspark.sql.functions as F

def processKafkaMessage(df):
    df.createOrReplaceTempView("vResultado")
    processedData = spark.sql("SELECT id, nombre, apellido, pais FROM vResultado")
    return processedData

if __name__ == "__main__":

    spark = SparkSession\
        .builder\
        .appName("KafkaIntegration")\
        .master("local[3]")\
        .config("spark.sql.shuffle.partitions", 3)\
        .getOrCreate()


    tiposStreamingDF = spark.readStream\
        .format("kafka")\
        .option("kafka.bootstrap.servers", "kafka1p:9092, kafka2p:9092, kafka3p:9092")\
        .option("subscribe", "DataTopic")\
        .option("startingOffsets", "earliest")\
        .load()
        
    schema = StructType([\
        StructField("id", IntegerType()),\
        StructField("nombre", StringType()),\
        StructField("apellido", StringType()),\
        StructField("edad", IntegerType()),\
        StructField("email", StringType()),\
        StructField("telefono", StringType()),\
        StructField("ubicacion",\
                StructType([\
                        StructField("direccion", StringType()),\
                        StructField("ciudad", StringType()),\
                        StructField("pais", StringType())\
                    ])
                   )
    ])
    
    parsedDF = tiposStreamingDF\
        .select("value")\
        .withColumn("value", F.col("value").cast(StringType()))\
        .withColumn("input", F.from_json(F.col("value"), schema))\
        .withColumn("id", F.col("input.id"))\
        .drop("value")\
        .withColumn("nombre", F.col("input.nombre"))\
        .withColumn("apellido", F.col("input.apellido"))\
        .withColumn("edad", F.col("input.edad"))\
        .withColumn("email", F.col("input.email"))\
        .withColumn("ubicacion", F.col("input.ubicacion"))\
        .drop("input")
        #.withColumn("direccion", F.col("input.ubicacion.direccion"))\
        #.withColumn("ciudad", F.col("input.ubicacion.ciudad"))\
        #.withColumn("pais", F.col("input.ubicacion.pais"))\
        #.drop("input")
        
        #.select("value")\
        #.selectExpr("CAST(value AS STRING)")\
        #.withColumn("value", F.from_json(F.col("value"), schema))\
        #.select("value.*")\
        #.withColumnRenamed("ubicacion.direccion", "direccion")\
        #.withColumnRenamed("ubicacion.ciudad", "ciudad")\
        #.withColumnRenamed("ubicacion.pais", "pais")\
        #.drop("value")
        

    #processedDataDF = processKafkaMessage(parsedDF)

    #clientesPaisesDf = processedDataDF.groupBy("pais").count()

    outputQuery = parsedDF.writeStream\
        .queryName("query_paises_kafka")\
        .outputMode("append")\
        .format("memory")\
        .start() #.outputMode("update")
    
    from time import sleep
    for x in range(50):
        # Consulta y muestra los resultados en la consola
        spark.sql("select * from query_paises_kafka").show(1000, False)
        sleep(1)

    print('llego hasta aqui')

    outputQuery.awaitTermination()
